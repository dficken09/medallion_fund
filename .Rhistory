results = data.table(input   = rownames(summary_OLS$coefficients),
est_OLS = summary_OLS$coefficients[, 1],
p_OLS   = summary_OLS$coefficients[, 4])
### LASSO
# make the model matrix for the LASSO
X = model.matrix(outcome_spend ~ 0 + .*W, data = training_DT)
y = training_DT$outcome_spend
#fit_LASSO = cv.glmnet(x=X, y=y, nfolds = 10, foldid = folds) # alpha = 1 is standard
#saveRDS(fit_LASSO, "fit_LASSO")
fit_LASSO = readRDS("fit_LASSO")
results[, est_LASSO := coef(fit_LASSO, s = "lambda.min")[,1]]
############ Non-parametric Estimator  ####################
### Causal Forest
# the *predicted* conditional average treatment effects from a causal forest are
# included in the file `Predicted-Causal-Forest-CATE.RData`.
# The causal forest was estimated using 1000 trees.
load("Predicted-Causal-Forest-CATE.RData")
predict_DT = crm_DT[training_sample == 0,
c("customer_id", "outcome_spend", "W"), with = FALSE]
### add predictions for OLS
predict_DT[, y_OLS := predict(fit_OLS, newdata = validation_DT)]
### add predictions for LASSO
new_X = model.matrix(outcome_spend ~ 0 + .*W, data = validation_DT)
predict_DT[,y_LASSO := predict(fit_LASSO, newx = new_X, s = "lambda.min")]
######## find individual heterogeneous treatment effects for the following:
### OLS
# change dataset so that W = 1
W_valid = copy(validation_DT)
W_valid[, W:= 1]
predict_W_OLS = predict(fit_OLS, newdata = W_valid)
# change dataset so that W = 0
no_W_valid = copy(validation_DT)
no_W_valid[, W:= 0]
predict_no_W_OLS = predict(fit_OLS, newdata = no_W_valid)
# find heterogeneous treatment effect usng OLS
tau_OLS = predict_W_OLS - predict_no_W_OLS
### LASSO
# create model.matrix with W = 1
X_with_W = model.matrix(outcome_spend ~ 0 + .*W, data = W_valid)
predict_W_LASSO = predict(fit_LASSO, newx = X_with_W, s = "lambda.min")
# create model.matrix with W = 0
X_no_W = model.matrix(outcome_spend ~ 0 + .*W, data = no_W_valid)
predict_no_W_LASSO = predict(fit_LASSO, newx = X_no_W, s = "lambda.min")
# find heterogeneous treatment effect usng LASSO
tau_LASSO = predict_W_LASSO - predict_no_W_LASSO
predict_DT = cbind.data.frame(predict_DT, tau_OLS, tau_LASSO)
### merge Causal Forest with predict_DT
predict_DT = merge(predict_DT, predict_DT_2017)
### tau_LASSO is just vector not named vector so have to provide column name
colnames(predict_DT)[7]<-"tau_LASSO"
saveRDS(predict_DT,"predict_DT")
mean_spend_0 = mean(predict_DT[W == 0, outcome_spend])
mean_spend_1 = mean(predict_DT[W == 1, outcome_spend])
ATE = mean_spend_1 - mean_spend_0
cat(mean_spend_0, mean_spend_1, ATE)
# Summarize and graph the distribution of the predicted heterogeneous treatment effects, tau_i, from the different estimation methods.
require(gridExtra)
hist_OLS<-ggplot(predict_DT, aes(x=tau_OLS)) +
geom_histogram(binwidth = 1, center = 0.5, color = "gray30", fill = "darkorchid3", alpha = 0.5) +
scale_x_continuous("CATE - OLS", limits = c(-40, 50)) +
scale_y_continuous("Frequency") + theme_bw()
hist_LASSO<-ggplot(predict_DT, aes(x=tau_LASSO)) +
geom_histogram(binwidth = 1, center = 0.5, color = "gray30", fill = "darkorchid3", alpha = 0.5) +
scale_x_continuous("CATE - LASSO", limits = c(-40, 50)) +
scale_y_continuous("Frequency") + theme_bw()
hist_cforest<-ggplot(predict_DT, aes(x=tau_cforest)) +
geom_histogram(binwidth = 1, center = 0.5, color = "gray30", fill = "darkorchid3", alpha = 0.5) +
scale_x_continuous("CATE - Causal Forest", limits = c(-40, 50)) +
scale_y_continuous("Frequency") + theme_bw()
grid.arrange(hist_OLS, hist_LASSO, hist_cforest, ncol=1, nrow=3)
cor_matrix = cor(predict_DT[, c("tau_OLS", "tau_LASSO", "tau_cforest"),
with = FALSE])
kable(cor_matrix)
### Use the lift table function from assignment 6 and modify it so we can calculate lifts given W = 0/1
liftTable <- function(model_name, w, y, score, N_groups = 10) {
DT = data.table(w     = w,
y     = y,
score = score)
DT[, score_group := as.integer(cut_number(score, n = N_groups))]
DT[, y_1 := ifelse(w==1, y, NA)]
DT[, y_0 := ifelse(w==1, NA, y)]
lift_DT = DT[, .(model      = model_name,
score      = mean(score),
y          = mean(y_1, na.rm=T) - mean(y_0,na.rm = T),
N          = .N,
std_error  = sqrt((sd(y_1, na.rm = T)^2)/(length(complete.cases(y_1))) +
(sd(y_0, na.rm = T)^2)/(length(complete.cases(y_0))))),
keyby = score_group]
lift_DT[, `:=`(lower  = y + qt(0.025, df = N-1)*std_error,
upper  = y + qt(0.975, df = N-1)*std_error)]
lift_DT[, c("std_error", "N") := NULL]
lift_DT[, lift := 100*y/mean(y)]
return(lift_DT)
}
lift_OLS <- liftTable("OLS Lift", predict_DT$W, predict_DT$outcome_spend, predict_DT$tau_OLS, 20)
lift_LASSO<- liftTable("LASSO Lift", predict_DT$W, predict_DT$outcome_spend, predict_DT$tau_LASSO, 20)
lift_cforest<-liftTable("Causal Forest Lift", predict_DT$W, predict_DT$outcome_spend, predict_DT$tau_cforest, 20)
lift_table = rbind.data.frame(lift_OLS, lift_LASSO, lift_cforest)
lift_table[, model := factor(model, levels = c("OLS Lift", "LASSO Lift","Causal Forest Lift"))]
kable(lift_table)
N_groups = 20
ggplot(lift_table, aes(x = score_group, y = y)) +
geom_errorbar(aes(ymin = lower, ymax = upper), color = "deepskyblue2",
size = 0.6, width = 0.1) +
geom_point(shape = 21, color = "gray30", fill = "hotpink", size = 2.5) +
scale_x_continuous("Score", limits = c(1, N_groups),
breaks = seq(0, N_groups, 5), minor_breaks = 1:N_groups) +
scale_y_continuous("Average Treatment Effects",
breaks = seq(0, 60, 10)) +
facet_wrap(~ model, ncol = 1) +
theme_bw()
margin = 0.325          # 32.5 percent
cost   = 0.99           # 99 cents
top_percent = seq(from = 0, to = 1, by = 0.01)
### grab the predictProfit function from solution in assignment 6 and modify it accordingly
predictProfitTopPercent <- function(model_name, top_percent, score, W, spend, margin, cost) {
# Observed profits for treated and untreated units
profit_0 = margin*spend
profit_1 = margin*spend - cost
# Output table
K = length(top_percent)
profits_DT = data.table(model_name  = model_name,
top_percent = top_percent,
profit      = rep(0.0, K))
scale_factor = 1000/length(W)
for (k in 1:K) {
if (top_percent[k] < 1e-12) {
threshold = max(score) + 1             # Make sure everyone is included
} else if (top_percent[k] > 1 - 1e-12) {
threshold = min(score) - 1             # Make sure nobody is included
} else {
threshold = quantile(score, probs = 1 - top_percent[k])
}
T = score >= threshold   # Indicator: Is a customer among the top percent?
N_0 = sum(1-T)           # Number of customers not among the top percent
N_1 = sum(T)             # Number of customers among the top percent
# Now calculate the mean profits for the treated and untreated units
mean_profit_0 = sum((1-T)*(1-W)*profit_0)/sum((1-T)*(1-W))
mean_profit_1 = sum(T*W*profit_1)/sum(T*W)
if (is.nan(mean_profit_0)) mean_profit_0 = 0.0
if (is.nan(mean_profit_1)) mean_profit_1 = 0.0
profits_DT[k, profit := scale_factor*(N_1*mean_profit_1 + N_0*mean_profit_0)]
}
return(profits_DT)
}
### OLS model profits
OLS_profits = predictProfitTopPercent("OLS", top_percent, predict_DT$tau_OLS, predict_DT$W, predict_DT$outcome_spend, margin, cost)
OLS_profits
OLS_opt_n_index = OLS_profits[which.max(OLS_profits$profit), top_percent]; OLS_opt_n_index
max(OLS_profits$profit)
### LASSO model profits
LASSO_profits = predictProfitTopPercent("LASSO", top_percent, predict_DT$tau_LASSO, predict_DT$W, predict_DT$outcome_spend, margin, cost)
LASSO_profits
LASSO_opt_n_index = OLS_profits[which.max(LASSO_profits$profit), top_percent]; LASSO_opt_n_index
max(LASSO_profits$profit)
### Causal Forest profits
cforest_profits = predictProfitTopPercent("Causal Forest", top_percent, predict_DT$tau_cforest, predict_DT$W, predict_DT$outcome_spend, margin, cost)
cforest_profits
cforest_opt_n_index = OLS_profits[which.max(cforest_profits$profit), top_percent]; cforest_opt_n_index
max(cforest_profits$profit)
##### combine the tables into one profit_table
profit_DT<-rbind.data.frame(OLS_profits, LASSO_profits, cforest_profits)
profit_DT[, model_name := factor(model_name, levels = c("OLS", "LASSO","Causal Forest"))]
ggplot(profit_DT, aes(x = top_percent, y = profit)) +
geom_hline(data = profit_DT[top_percent == 0, .(model_name, profit_0 = profit)],
aes(yintercept = profit_0), color = "slategray3", size = 1) +
geom_line(color = "mediumvioletred", size = 1) +
scale_x_continuous("Percent targeted", limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous("Profit", limits = c(1800, 2400),
breaks = seq(1800, 2400, 50)) +
theme_bw() +
facet_wrap(~ model_name, nrow = 3)
#load("/classes/3710501_spr2019/Data/Assignment-7/Randomized-Implementation-Sample-2018.RData")
load("Randomized-Implementation-Sample-2018.RData")
# change mailing_indicator to "W" as we did before
setnames(crm_DT, "mailing_indicator", "W")
# create new predict_DT datatable to store predictions vs. actual results
new_predict_DT = crm_DT[,c("customer_id", "outcome_spend", "W"), with = FALSE]
####### 1. Use the model predictions based **only on the 2017 data**. #######
# fit_OLS
# fit_LASSO
# causal forest given to us
####### 2. Predict heterogeneous treatment effects for the customers in the October 2018 data. #######
### OLS prediction
# need to match colnames in 2018 data to validation_DT colnames for prediction
X_2018 = subset(crm_DT, select = names(validation_DT))
X_W_2018 = copy(X_2018)
X_no_W_2018 = copy(X_2018)
X_W_2018[, W:= 1]
X_no_W_2018[, W:=0]
predict_W_2018 = predict(fit_OLS, newdata = X_W_2018)
predict_no_W_2018 = predict(fit_OLS, newdata = X_no_W_2018)
# find heterogeneous treatment effect usng OLS
tau_OLS = predict_W_2018 - predict_no_W_2018
### LASSO prediction
# create model.matrix with W = 1
X_with_W = model.matrix(outcome_spend ~ 0 + .*W, data = X_W_2018)
predict_W_LASSO = predict(fit_LASSO, newx = X_with_W, s = "lambda.min")
# create model.matrix with W = 0
X_no_W = model.matrix(outcome_spend ~ 0 + .*W, data = X_no_W_2018)
predict_no_W_LASSO = predict(fit_LASSO, newx = X_no_W, s = "lambda.min")
# find heterogeneous treatment effect usng LASSO
tau_LASSO = predict_W_LASSO - predict_no_W_LASSO
new_predict_DT<-cbind.data.frame(new_predict_DT, tau_OLS, tau_LASSO)
### Causal prediction - need to merge new_predict_DT with predict_DT_2018
new_predict_DT<-merge(new_predict_DT, predict_DT_2018)
setnames(new_predict_DT, "1", "tau_LASSO")
### sanity check on calculations
cor_matrix = cor(new_predict_DT[, c("tau_OLS", "tau_LASSO", "tau_cforest"),
with = FALSE])
kable(cor_matrix)
######## 3. Evaluate the model predictions using the 2018 data. #######
# create lift tables to examine different models' abilities to predict CATE
new_lift_OLS <- liftTable("OLS Lift", new_predict_DT$W, new_predict_DT$outcome_spend, new_predict_DT$tau_OLS, 20)
new_lift_LASSO<- liftTable("LASSO Lift", new_predict_DT$W, new_predict_DT$outcome_spend, new_predict_DT$tau_LASSO, 20)
new_lift_cforest<-liftTable("Causal Forest Lift", new_predict_DT$W, new_predict_DT$outcome_spend, new_predict_DT$tau_cforest, 20)
new_lift_table = rbind.data.frame(new_lift_OLS, new_lift_LASSO, new_lift_cforest)
new_lift_table[, model := factor(model, levels = c("OLS Lift", "LASSO Lift","Causal Forest Lift"))]
kable(new_lift_table)
# graph the lifts for the 2018 data
N_groups = 20
ggplot(new_lift_table, aes(x = score_group, y = y)) +
geom_errorbar(aes(ymin = lower, ymax = upper), color = "deepskyblue2",
size = 0.6, width = 0.1) +
geom_point(shape = 21, color = "gray30", fill = "hotpink", size = 2.5) +
scale_x_continuous("Score", limits = c(1, N_groups),
breaks = seq(0, N_groups, 5), minor_breaks = 1:N_groups) +
scale_y_continuous("Average Treatment Effects",
breaks = seq(0, 60, 10)) +
facet_wrap(~ model, ncol = 1) +
theme_bw()
### use the predictProfit function to examine profits using 2018 data
### OLS model profits
OLS_profits = predictProfitTopPercent("OLS", top_percent, new_predict_DT$tau_OLS, new_predict_DT$W, new_predict_DT$outcome_spend, margin, cost)
OLS_profits
new_OLS_opt_n_index = OLS_profits[which.max(OLS_profits$profit), top_percent]; new_OLS_opt_n_index
max(OLS_profits$profit)
### LASSO model profits
LASSO_profits = predictProfitTopPercent("LASSO", top_percent, new_predict_DT$tau_LASSO, new_predict_DT$W, new_predict_DT$outcome_spend, margin, cost)
LASSO_profits
new_LASSO_opt_n_index = OLS_profits[which.max(LASSO_profits$profit), top_percent]; new_LASSO_opt_n_index
max(LASSO_profits$profit)
### Causal Forest profits
cforest_profits = predictProfitTopPercent("Causal Forest", top_percent, new_predict_DT$tau_cforest, new_predict_DT$W, new_predict_DT$outcome_spend, margin, cost)
cforest_profits
new_cforest_opt_n_index = OLS_profits[which.max(cforest_profits$profit), top_percent]; new_cforest_opt_n_index
max(cforest_profits$profit)
##### combine the tables into one profit_table
new_profit_DT<-rbind.data.frame(OLS_profits, LASSO_profits, cforest_profits)
new_profit_DT[, model_name := factor(model_name, levels = c("OLS", "LASSO","Causal Forest"))]
ggplot(new_profit_DT, aes(x = top_percent, y = profit)) +
geom_hline(data = new_profit_DT[top_percent == 0, .(model_name, profit_0 = profit)],
aes(yintercept = profit_0), color = "slategray3", size = 1) +
geom_line(color = "mediumvioletred", size = 1) +
scale_x_continuous("Percent targeted", limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous("Profit", limits = c(1600, 2100),
breaks = seq(1600, 2100, 50)) +
theme_bw() +
facet_wrap(~ model_name, nrow = 3)
# show new vs. old best "top percent n"
new_OLS_opt_n_index; OLS_opt_n_index
OLS_profits[OLS_opt_n_index,]
OLS_profits[OLS_opt_n_index,]
OLS_profits[OLS_opt_n_index,]
OLS_profits
OLS_opt_n_index
OLS_profits[which(OLS_profits$top_percent==OLS_opt_n_index),]
#### use prior percentages determined in 2017 data to compare three models profits
#### OLS profit ####
# show old vs. new best "top percent n"
OLS_opt_n_index; new_OLS_opt_n_index
# show profits at the old, predetermined "best percentage"
OLS_profits[which(OLS_profits$top_percent==OLS_opt_n_index),]
#### LASSO profit ####
# show old vs. new best "top percent n"
LASSO_opt_n_index; new_LASS_opt_n_index
#### use prior percentages determined in 2017 data to compare three models profits
#### OLS profit ####
# show old vs. new best "top percent n"
OLS_opt_n_index; new_OLS_opt_n_index
# show profits at the old, predetermined "best percentage"
OLS_profits[which(OLS_profits$top_percent==OLS_opt_n_index),]
#### LASSO profit ####
# show old vs. new best "top percent n"
LASSO_opt_n_index; new_LASSO_opt_n_index
# show profits at the old, predetermined "best percentage"
LASSO_profits[which(LASSO_profits$top_percent==LASSO_opt_n_index),]
#### Causal Forest profit ####
# show old vs. new best "top percent n"
cforest_opt_n_index; new_cforest_opt_n_index
# show profits at the old, predetermined "best percentage"
cforest_profits[which(cforest_profits$top_percent==cforest_opt_n_index),]
### find top n = 0, top n = 100 profits
head(new_profit_DT,1)
tail(new_profit_DT,1)
2385-1886
2023-1683
#### use prior percentages determined in 2017 data to compare three models profits
#### OLS profit ####
# show old vs. new best "top percent n"
OLS_opt_n_index; new_OLS_opt_n_index
# show profits at the old, predetermined "best percentage"
OLS_profits[which(OLS_profits$top_percent==OLS_opt_n_index),]
#### LASSO profit ####
# show old vs. new best "top percent n"
LASSO_opt_n_index; new_LASSO_opt_n_index
# show profits at the old, predetermined "best percentage"
LASSO_profits[which(LASSO_profits$top_percent==LASSO_opt_n_index),]
#### Causal Forest profit ####
# show old vs. new best "top percent n"
cforest_opt_n_index; new_cforest_opt_n_index
# show profits at the old, predetermined "best percentage"
cforest_profits[which(cforest_profits$top_percent==cforest_opt_n_index),]
shiny::runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp()
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp()
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
shiny::runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
62000*1.5*10^6
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
tagList("URL link:", a("CAPM - Investopedia", href="https://www.investopedia.com/terms/c/capm.asp")),
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
runApp('Google Drive/Medallion Fund/data_grabber')
shiny::runApp('Google Drive/Medallion Fund/data_grabber')
setwd("~/Google Drive/Medallion Fund/app_files")
shiny::runApp()
source('~/Google Drive/Medallion Fund/app_files/finviz.R', echo=TRUE)
get_stock_data("AAPL")
source('~/Google Drive/Medallion Fund/app_files/finviz.R', echo=TRUE)
get_stock_data("AAPL")
source('~/Google Drive/Medallion Fund/app_files/s_p_data.R', echo=TRUE)
help("Deprecated")
getSymbols("ABMD")
source('~/Google Drive/Medallion Fund/app_files/s_p_data.R', echo=TRUE)
install.packages("xml2")
install.packages("xml2")
source('~/Google Drive/Medallion Fund/app_files/finviz.R', echo=TRUE)
source('~/Google Drive/Medallion Fund/app_files/s_p_data.R', echo=TRUE)
getSymbols("ADI", from = as.Date("1970-01-01"))
getSymbols("AAP", from = as.Date("1970-01-01"))
library(quantmod)
library(rvest)
library(data.table)
library(finreportr)
### grab get_stock_data function from finviz
source("finviz.R")
url <- "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
SP500 <- url %>%
html() %>%
html_nodes(xpath='//*[@id="mw-content-text"]/div/table[1]') %>%
html_table()
SP500 <- SP500[[1]]
symbol_list = SP500$Symbol
sp500 = new.env()
### convert "." to "-"
symbol_list <-unlist(lapply(symbol_list, function(x) gsub("[.]","-",x)))
for (i in symbol_list){
getSymbols(i, env = sp500, from = as.Date("1970-01-01"))
}
getSymbols("MMM",from = as.Date("1970-01-01"))
MMM <-as.data.table(MMM)
sp_dt = data.table(MMM$index)
colnames(sp_dt)<-"Date"
setkey(sp_dt, Date)
for (i in symbol_list){
temp = data.frame(mget(i, envir = sp500))
temp_v2 = as.data.table(data.frame(Date = row.names(temp), coredata(temp)))
temp_v2$Date<-as.Date(temp_v2$Date)
setkey(temp_v2, Date)
sp_dt<-merge(sp_dt,temp_v2, by = "Date", all.x = T)
}
### turn into list of lists, titled by the ticker name
sp_list = list()
for (i in 0:(((length(sp_dt)-1)/6)-1)){
temp = sp_dt[, c(2+6*i):(7+6*i)]
temp = cbind.data.frame(sp_dt[,1],temp)
list_name = substr(colnames(temp)[2],1, gregexpr(pattern = "[.]",colnames(temp)[2])[[1]][1][1]-1)
colnames(temp)<-c("Date","Open","High","Low","Close","Volume","Adjusted")
header = SP500[which(SP500$Symbol==list_name),]
list_df<-list(header = header, ts_data = temp)
sp_list[[list_name]][[1]]<- header
sp_list[[list_name]][[2]]<- temp
}
saveRDS(sp_list, "sp_list")
### get all of the stock info in 1 large dataframe
stock_db = data.frame()
for (i in symbol_list){
temp = get_stock_data(i)
stock_db=rbind.data.frame(stock_db, temp)
}
saveRDS(stock_db,"stock_db")
stock_db
runApp()
str(stock_db)
row.names(stock_db)
temp = stock_db
temp = data.frame(lapply(temp, as.character()), stringsAsFactors = F)
temp = data.frame(lapply(temp, as.character), stringsAsFactors = F)
str(temp)
row.names(temp)
stock_db
get_stock_data("BKR")
symbol_list
SP500$Symbol
stock_db = data.frame()
for (i in symbol_list){
if (i != "BKR"){
temp = get_stock_data(i)
stock_db=rbind.data.frame(stock_db, temp)
}
}
source('~/Google Drive/Medallion Fund/app_files/s_p_data.R', echo=TRUE)
str(stock_db)
row.names(stock_db)
row_n <- row.names(stock_db)
stock_db = data.frame(lapply(temp, as.character), stringsAsFactors = F)
row.names(stock_db)<-row_n
stock_db
temp<-readRDS(stock_db)
temp<-readRDS("stock_db")
str(temp)
stock_db<-readRDS("stock_db")
row_n <- row.names(stock_db)
stock_db = data.frame(lapply(stock_db, as.character), stringsAsFactors = F)
row.names(stock_db)<-row_n
str(stock_db)
stock_db
saveRDS(stock_db,"stock_db")
runApp()
source('~/Google Drive/Medallion Fund/app_files/sector_analyzer.R', echo=TRUE)
source('~/Google Drive/Medallion Fund/app_files/sector_analyzer.R', echo=TRUE)
sector_list
sector_list$Industrials
sum_temp = sector_list$Industrials
sum_temp
?gsub()
### Change Column Names "Market.Cap" and "Avg.Volume" to "Market Cap" and "Avg Volume", respectively
gsub("Market.Cap", "Market Cap", colnames(stock_db))
### Change Column Names "Market.Cap" and "Avg.Volume" to "Market Cap" and "Avg Volume", respectively
colnames(stock_db)<-gsub("Market.Cap", "Market Cap", colnames(stock_db))
colnames(stock_db)<-gsub("Avg.Volume", "Avg Volume", colnames(stock_db))
colnames(stock_db)
source('~/Google Drive/Medallion Fund/app_files/sector_analyzer.R', echo=TRUE)
saveRDS(stock_db,"stock_db")
str(stock_db)
### Change Column Names "Market.Cap" and "Avg.Volume" to "Market Cap" and "Avg Volume", respectively
colnames(stock_db)<-gsub("Market.Cap", "Market Cap", colnames(stock_db))
colnames(stock_db)<-gsub("Avg.Volume", "Avg Volume", colnames(stock_db))
saveRDS(stock_db,"stock_db")
str(stock_db)
source('~/Google Drive/Medallion Fund/app_files/sector_analyzer.R', echo=TRUE)
runApp()
### Change Column Names "Book.sh" and "P.E" to "Book/sh" and "P/E", respectively
colnames(stock_db)<-gsub("Book.sh", "Book/sh", colnames(stock_db))
colnames(stock_db)<-gsub("P.E", "P/E", colnames(stock_db))
saveRDS(stock_db,"stock_db")
runApp()
?rsconnect::deployApp()
rsconnect::deployApp('app_files')
setwd("~/Google Drive/Medallion Fund")
rsconnect::deployApp('app_files', size='xlarge')
rsconnect::configureApp('app_files', size='xlarge')
setwd("~/Google Drive/Medallion Fund/app_files")
rsconnect::configureApp('app.R', size='xlarge')
rsconnect::configureApp('app_files', size='xlarge')
runApp()
rsconnect::deployApp('app_files', size='xlarge')
rsconnect::deployApp('app_files')
rsconnect::configureApp('app_files', size='xlarge')
rsconnect::deployApp(upload=FALSE)
runApp()
library(rsconnect)
rsconnect::deployApp(server = "shinyapps.io")
